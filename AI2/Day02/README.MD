# Day 01  

### 1. 머신러닝  

------------------------------------

### 1) 지도학습
* 문제와 정답을 `모두 알려주고 학습`시키는 것으로 `반복학습`을 통해 오류를 줄여나가면서 점차 정답에 가까워지는 방법이다.  
  * `분류` : 주어진 데이터가 어느 클래스에 속하는지 판별하는 것  
  * `회귀` : 전달된 데이터를 바탕으로 값을 예상한다.  

------------------------------------

### 2) 비지도학습  
* `답을 가르쳐주지 않고` 예측하는 방법  
  * 클러스터링(Clustering) : 전달된 데이터를 어떤 규칙에 따라서 나누는 것  
    * Kmeans : 몇 개의 클러스터를 분할할 지 사전에 정해둔 경우 추천  
    * 스펙트럼 클러스터링(GMM) : Kmeans로 분석되지 않는 경우 이용, 비선형적인 클러스터링 분석 방법  
    * MeanShift, VGBGMM : 몇 개의 클러스터를 분할할 지 사전에 정할 수 없는 경우 추천  
  * 차원 축소(Dimensioinality Reduction) : 데이터의 고차원 특성을 저차원으로 압축하여 데이터를 시각화하거나 다른 분석작업에 활용하는데 사용  
    * 주성분분석(PCA, Principal Component Anaysis) : 데이터의 주성분을 추출하여 데이터의 차원을 줄이는 기법  
    * t-SNE : 비선형 차원 축소 기법으로, 고차원 데이터를 특히 2, 3차원 등으로 줄여 가시화화는데 유용하게 사용되는 기법  
    * LLE(Locally Linear Embedding) :  
  * 연관규칙학습(Association Rule Learning) : 데이터 세트에서 항목들 사이의 흥미로운 관계나 패턴을 찾는 작업  
    * Apriori(알라희) : 빈발 항목 집합을 찾는데 사용  

--------------------------------------

### 3) 강화학습  
* 주어진 환경과 상호작용하면서 보상을 최대화하기 위한 행동을 학습하는 방법  
  * 용어정리  
    * 에이전트(Agent) : 행동을 수행하는 주체  
    * 환경(Environment) : 에이전트가 상호작용하는 외부환경, 다양한 상태와 보상이 발생  
    * 상태(State) : 에이전트와 환경의 상호작용을 통해 특정시점에서의 환경상태를 나타냄  
    * 행동(Action) : 에이전트가 특정 상태에서 취할 수 있는 선택가능한 행동의 집합  
    * 보상(Reword) : 에이전트가 특정 행동을 수행한 결과 받는 보상신호  
  * Q-Learning : 에이전트가 환경과 상호작용하면서 최적의 Q-함수를 찾아가면서 최적의 행동을 선택하도록 하는 학습 방법  
  * SARSA : 상태 - 행동 - 보상 - 상태 - 행동 순서로 학습하는 방법  







